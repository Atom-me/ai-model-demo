"""
AIHubMixä»£ç ç”Ÿæˆæµ‹è¯•è„šæœ¬
"""
import os
import sys
import time
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from platforms.aihubmix import AIHubMixClient
from config.config import Config

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_code_generation():
    """æµ‹è¯•AIHubMixä»£ç ç”ŸæˆåŠŸèƒ½"""
    print("ğŸš€ AIHubMixä»£ç ç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    
    if not Config.AIHUBMIX_API_KEY:
        print("âŒ AIHubMix APIå¯†é’¥æœªé…ç½®")
        return
    
    print(f"ğŸ”‘ APIå¯†é’¥: {Config.AIHUBMIX_API_KEY[:8]}...")
    print(f"ğŸŒ APIç«¯ç‚¹: {Config.AIHUBMIX_BASE_URL}")
    print()
    
    try:
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = AIHubMixClient()
        print("âœ… AIHubMixå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print()
        
        # ä»£ç ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ - åªæµ‹è¯•Vue.jsç™»å½•è¡¨å•
        test_cases = [
            {
                "name": "Vue.jsç™»å½•è¡¨å•",
                "prompt": """ä½¿ç”¨Vue.jsåˆ›å»ºä¸€ä¸ªç®€å•çš„ç™»å½•è¡¨å•ï¼Œè¦æ±‚å¦‚ä¸‹ï¼š
1. åŒ…å«ç”¨æˆ·åå’Œå¯†ç è¾“å…¥æ¡†
2. åŒ…å«ç™»å½•æŒ‰é’®
3. æ·»åŠ åŸºæœ¬çš„è¡¨å•éªŒè¯ï¼ˆç”¨æˆ·åä¸èƒ½ä¸ºç©ºï¼Œå¯†ç é•¿åº¦è‡³å°‘6ä½ï¼‰
4. éªŒè¯å¤±è´¥æ—¶æ˜¾ç¤ºé”™è¯¯æç¤º
5. ç™»å½•æˆåŠŸæ—¶æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
6. ä½¿ç”¨Vue 3çš„ç»„åˆå¼API
7. åŒ…å«åŸºæœ¬çš„CSSæ ·å¼""",
                "max_tokens": 1500
            }
        ]
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"ğŸ“ æµ‹è¯• {i}: {test_case['name']}")
            print(f"ğŸ’­ æç¤º: {test_case['prompt'][:50]}...")
            print(f"ğŸ¯ æœ€å¤§tokens: {test_case['max_tokens']}")
            
            try:
                # å‘é€ä»£ç ç”Ÿæˆè¯·æ±‚ - å¯ä»¥å°è¯•ä¸åŒæ¨¡å‹
                # å¯é€‰æ¨¡å‹: "gpt-5", "gpt-4o", "gpt-4-turbo", "claude-3-sonnet"
                model_to_use = "gpt-4o"  # å…ˆå°è¯•gpt-4oï¼Œå¦‚æœéœ€è¦GPT-5å†æ”¹å›æ¥
                
                print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_to_use}")
                
                start_time = time.time()
                
                response = client.chat(
                    message=test_case['prompt'],
                    model=model_to_use,
                    max_tokens=test_case['max_tokens'],  # gpt-4oä½¿ç”¨max_tokens
                    temperature=0.2,  # ä»£ç ç”Ÿæˆä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥ç¡®ä¿å‡†ç¡®æ€§
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆï¼Œç²¾é€šVue.jsã€‚è¯·ç”Ÿæˆé«˜è´¨é‡ã€å¯è¿è¡Œçš„ä»£ç ï¼ŒåŒ…å«é€‚å½“çš„æ³¨é‡Šã€‚ä»£ç åº”è¯¥éµå¾ªVue.jsæœ€ä½³å®è·µã€‚"
                )
                
                end_time = time.time()
                response_time = end_time - start_time
                
                if response['success']:
                    print(f"âœ… ç”ŸæˆæˆåŠŸ - å“åº”æ—¶é—´: {response_time:.2f}s")
                    
                    # æ£€æŸ¥å†…å®¹é•¿åº¦
                    content = response.get('content', '')
                    if not content or content.strip() == '':
                        print("âš ï¸  è­¦å‘Š: å“åº”å†…å®¹ä¸ºç©ºï¼")
                        print("ğŸ” è°ƒè¯•ä¿¡æ¯:")
                        
                        # æ˜¾ç¤ºåŸå§‹å“åº”è°ƒè¯•ä¿¡æ¯
                        if 'raw_response' in response:
                            raw_resp = response['raw_response']
                            print(f"  - å“åº”å¯¹è±¡ç±»å‹: {type(raw_resp)}")
                            print(f"  - Choicesæ•°é‡: {len(raw_resp.choices) if hasattr(raw_resp, 'choices') else 'N/A'}")
                            
                            if hasattr(raw_resp, 'choices') and raw_resp.choices:
                                choice = raw_resp.choices[0]
                                print(f"  - Messageç±»å‹: {type(choice.message) if hasattr(choice, 'message') else 'N/A'}")
                                print(f"  - Messageå†…å®¹: '{choice.message.content}'" if hasattr(choice, 'message') else 'N/A')
                                print(f"  - Finish reason: {choice.finish_reason}" if hasattr(choice, 'finish_reason') else 'N/A')
                        
                        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                        print("  1. GPT-5æ¨¡å‹å¯èƒ½éœ€è¦ä¸åŒçš„å‚æ•°é…ç½®")
                        print("  2. è¯·æ±‚è¢«å†…å®¹è¿‡æ»¤å™¨æ‹¦æˆª")
                        print("  3. APIè¿”å›æ ¼å¼å‘ç”Ÿå˜åŒ–")
                        print("  4. æ¨¡å‹æ­£åœ¨å¤„ç†ä¸­ä½†æœªå®Œæˆ")
                    else:
                        # æ˜¾ç¤ºå®Œæ•´çš„ç”Ÿæˆä»£ç 
                        print(f"ğŸ“„ å®Œæ•´ç”Ÿæˆä»£ç :")
                        print("=" * 60)
                        print(response['content'])
                        print("=" * 60)
                    
                    # æ˜¾ç¤ºä½¿ç”¨æƒ…å†µ
                    if response.get('usage'):
                        usage = response['usage']
                        print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={usage.get('prompt_tokens', 'N/A')}, "
                              f"è¾“å‡º={usage.get('completion_tokens', 'N/A')}, "
                              f"æ€»è®¡={usage.get('total_tokens', 'N/A')}")
                    
                    results.append({
                        'name': test_case['name'],
                        'success': True,
                        'response_time': response_time,
                        'content_length': len(response['content']),
                        'usage': response.get('usage'),
                        'model_used': model_to_use
                    })
                    
                else:
                    print(f"âŒ ç”Ÿæˆå¤±è´¥: {response['error']}")
                    results.append({
                        'name': test_case['name'],
                        'success': False,
                        'error': response['error'],
                        'model_used': model_to_use
                    })
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¼‚å¸¸: {str(e)}")
                results.append({
                    'name': test_case['name'],
                    'success': False,
                    'error': str(e),
                    'model_used': model_to_use if 'model_to_use' in locals() else 'Unknown'
                })
            
            print()
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(test_cases):
                time.sleep(1)
        
        # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
        print("=" * 60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print("=" * 60)
        
        successful_tests = [r for r in results if r['success']]
        failed_tests = [r for r in results if not r['success']]
        
        print(f"\nâœ… æˆåŠŸæµ‹è¯•: {len(successful_tests)}/{len(test_cases)}")
        for result in successful_tests:
            print(f"  ğŸ“ {result['name']}: {result.get('model_used', 'Unknown')} - "
                  f"{result['response_time']:.2f}s, {result['content_length']} å­—ç¬¦")
        
        if failed_tests:
            print(f"\nâŒ å¤±è´¥æµ‹è¯•: {len(failed_tests)}")
            for result in failed_tests:
                print(f"  ğŸ“ {result['name']}: {result.get('model_used', 'Unknown')} - {result['error']}")
        
        if successful_tests:
            avg_time = sum(r['response_time'] for r in successful_tests) / len(successful_tests)
            avg_length = sum(r['content_length'] for r in successful_tests) / len(successful_tests)
            print(f"\nğŸ“ˆ å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}s")
            print(f"ğŸ“ å¹³å‡ä»£ç é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
        
        print(f"\nğŸ’¡ ä»£ç ç”Ÿæˆå»ºè®®:")
        print("- ä½¿ç”¨è¾ƒä½çš„temperature (0.1-0.3) ç¡®ä¿ä»£ç å‡†ç¡®æ€§")
        print("- æ ¹æ®ä»£ç å¤æ‚åº¦è°ƒæ•´max_tokens (500-2000)")
        print("- æ·»åŠ system_promptæŒ‡å®šä»£ç é£æ ¼å’Œè¦æ±‚")
        print("- å¯¹äºå¤æ‚åŠŸèƒ½ï¼Œå¯ä»¥åˆ†æ­¥éª¤ç”Ÿæˆ")
        
    except Exception as e:
        print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–æˆ–æµ‹è¯•å¼‚å¸¸: {e}")

def interactive_code_generation():
    """äº¤äº’å¼ä»£ç ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("ğŸ¯ äº¤äº’å¼ä»£ç ç”Ÿæˆæ¨¡å¼")
    print("=" * 60)
    print("è¾“å…¥ä»£ç éœ€æ±‚ï¼ŒAIå°†ä¸ºä½ ç”Ÿæˆä»£ç ")
    print("è¾“å…¥ 'quit' é€€å‡º\n")
    
    if not Config.AIHUBMIX_API_KEY:
        print("âŒ AIHubMix APIå¯†é’¥æœªé…ç½®")
        return
    
    try:
        client = AIHubMixClient()
        
        while True:
            try:
                user_input = input("ğŸ‘¨â€ğŸ’» è¯·æè¿°ä½ éœ€è¦çš„ä»£ç : ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                if not user_input:
                    continue
                
                # è·å–ä»£ç ç±»å‹å’Œå¤æ‚åº¦
                print("\nğŸ”§ ä»£ç ç”Ÿæˆé…ç½®:")
                
                # é€‰æ‹©æœ€å¤§tokens
                print("é€‰æ‹©ä»£ç å¤æ‚åº¦:")
                print("1. ç®€å• (500 tokens) - ç®€å•å‡½æ•°æˆ–ä»£ç ç‰‡æ®µ")
                print("2. ä¸­ç­‰ (1000 tokens) - å®Œæ•´åŠŸèƒ½æ¨¡å—")
                print("3. å¤æ‚ (2000 tokens) - å¤æ‚ç®—æ³•æˆ–å¤šä¸ªç±»")
                
                complexity = input("è¯·é€‰æ‹© (1-3, é»˜è®¤2): ").strip()
                
                max_tokens_map = {'1': 500, '2': 1000, '3': 2000}
                max_tokens = max_tokens_map.get(complexity, 1000)
                
                print(f"\nğŸš€ æ­£åœ¨ç”Ÿæˆä»£ç ... (æœ€å¤§tokens: {max_tokens})")
                
                start_time = time.time()
                response = client.chat(
                    message=user_input,
                    max_tokens=max_tokens,
                    temperature=0.2,
                    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¨‹åºå‘˜ã€‚è¯·ç”Ÿæˆé«˜è´¨é‡ã€å¯è¿è¡Œçš„ä»£ç ï¼ŒåŒ…å«é€‚å½“çš„æ³¨é‡Šå’Œé”™è¯¯å¤„ç†ã€‚ä»£ç åº”è¯¥éµå¾ªæœ€ä½³å®è·µã€‚"
                )
                end_time = time.time()
                
                if response['success']:
                    print(f"\nâœ… ä»£ç ç”Ÿæˆå®Œæˆ! (ç”¨æ—¶: {end_time - start_time:.2f}s)")
                    print("=" * 50)
                    print(response['content'])
                    print("=" * 50)
                    
                    if response.get('usage'):
                        usage = response['usage']
                        print(f"\nğŸ“Š ä½¿ç”¨æƒ…å†µ: {usage.get('total_tokens', 'N/A')} tokens")
                else:
                    print(f"âŒ ç”Ÿæˆå¤±è´¥: {response['error']}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                
            print("\n" + "-" * 60 + "\n")
    
    except Exception as e:
        print(f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")

if __name__ == "__main__":
    # åªè¿è¡ŒVue.jsç™»å½•è¡¨å•æµ‹è¯•
    test_code_generation()